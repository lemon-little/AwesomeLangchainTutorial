{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638b1841",
   "metadata": {},
   "source": [
    "## ËÆ∞ÂøÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa80362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        # model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "        model=\"Qwen/Qwen3-8B\",\n",
    "        # Á°ÖÂü∫ÊµÅÂä®\n",
    "        api_key=\"sk-jvjyawqpodlkxlywatvemcdykkrbvthhjyjyapyvtnifwlbl\",\n",
    "        base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "        # # modelscope \n",
    "        # base_url=\"https://api-inference.modelscope.cn/v1/\",\n",
    "        # api_key=\"ms-e2666046-2f3b-4c76-bcc0-e21f8ebf9ea1\",\n",
    "    )\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3c9fd",
   "metadata": {},
   "source": [
    "# Add memory\n",
    "## 1. Create a MemorySaver checkpointer\n",
    "ËøôÊòØÂÜÖÂ≠ò‰∏≠ÁöÑcheckpointerÔºåËøôÂØπÊú¨ÊïôÁ®ãÊù•ËØ¥ÂæàÊñπ‰æø„ÄÇ‰ΩÜÊòØÔºåÂú®Áîü‰∫ßÂ∫îÁî®Á®ãÂ∫è‰∏≠ÔºåÊÇ®ÂèØËÉΩ‰ºöÂ∞ÜÂÖ∂Êõ¥Êîπ‰∏∫‰ΩøÁî® SqliteSaver Êàñ PostgresSaver Âπ∂ËøûÊé•Êï∞ÊçÆÂ∫ì„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70199bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "memory = InMemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ec63b",
   "metadata": {},
   "source": [
    "## 2. Compile the graph\n",
    "‰ΩøÁî®Êèê‰æõÁöÑcheckpointerÁºñËØëÂõæÔºåËØ•checkpointerÂ∞ÜÂú®ÂõæÈÄöËøáÊØè‰∏™ËäÇÁÇπÊó∂Ê£ÄÊü• StateÔºö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d364fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a903809",
   "metadata": {},
   "source": [
    "## 3. Interact with your chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5fe31d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there! My name is Will.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\n",
      "\n",
      "Hello, Will! Nice to meet you. How can I assist you today? üòä\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "user_input = \"Hi there! My name is Will.\"\n",
    "\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09119cdf",
   "metadata": {},
   "source": [
    "## 4. Ask a follow up question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a28d6d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\n",
      "\n",
      "Of course, Will! I remember your name. How can I assist you today? üòä\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Remember my name?\"\n",
    "\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e659de52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\n",
      "\n",
      "I don't remember your name, but I'd be happy to learn it! Would you like to provide your name so I can keep track of it for future interactions?\n"
     ]
    }
   ],
   "source": [
    "# The only difference is we change the `thread_id` here to \"2\" instead of \"1\"\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    {\"configurable\": {\"thread_id\": \"2\"}},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fddf35",
   "metadata": {},
   "source": [
    "## 5. Inspect the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc20f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='Hi there! My name is Will.', additional_kwargs={}, response_metadata={}, id='2f8aea9d-e649-4c75-af3f-504f10f741c1'), AIMessage(content='\\n\\nHello, Will! Nice to meet you. How can I assist you today? üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 215, 'prompt_tokens': 1825, 'total_tokens': 2040, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 196, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '0198e4ce71dc5c7a1dc975e16a94005e', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--1782db79-246e-4313-9ab0-7aa7a0ef79c9-0', usage_metadata={'input_tokens': 1825, 'output_tokens': 215, 'total_tokens': 2040, 'input_token_details': {}, 'output_token_details': {'reasoning': 196}}), HumanMessage(content='Remember my name?', additional_kwargs={}, response_metadata={}, id='9269eba4-40b0-4938-a046-301ceacee891'), AIMessage(content='\\n\\nOf course, Will! I remember your name. How can I assist you today? üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 392, 'prompt_tokens': 1857, 'total_tokens': 2249, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 372, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen3-8B', 'system_fingerprint': '', 'id': '0198e4ce9959b0c99dd485311da328fc', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--7f1b6df3-a141-4845-8505-f1406556cb5c-0', usage_metadata={'input_tokens': 1857, 'output_tokens': 392, 'total_tokens': 2249, 'input_token_details': {}, 'output_token_details': {'reasoning': 372}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0823be-1146-63c4-8004-2a5ca1cfe1e7'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-08-26T05:16:53.937642+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0823bd-940f-64fa-8003-147b1d375839'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9249c703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot.next  # (since the graph ended this turn, `next` is empty. If you fetch a state from within a graph invocation, next tells which node will execute next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e539338",
   "metadata": {},
   "source": [
    "# ÂÆåÊï¥‰ª£Á†Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d266f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "# from langchain.chat_models import init_chat_model\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        # model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "        model=\"Qwen/Qwen3-8B\",\n",
    "        # Á°ÖÂü∫ÊµÅÂä®\n",
    "        api_key=\"sk-jvjyawqpodlkxlywatvemcdykkrbvthhjyjyapyvtnifwlbl\",\n",
    "        base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "        # # modelscope \n",
    "        # base_url=\"https://api-inference.modelscope.cn/v1/\",\n",
    "        # api_key=\"ms-e2666046-2f3b-4c76-bcc0-e21f8ebf9ea1\",\n",
    "    )\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "memory = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f33d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there! My name is Will.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\n",
      "\n",
      "Hello, Will! Nice to meet you. How can I assist you today? üòä\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "user_input = \"Hi there! My name is Will.\"\n",
    "\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90271f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\n",
      "\n",
      "Of course, Will! I'm glad to see you again. How can I assist you today? üòä\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Remember my name?\"\n",
    "\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f25aa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\n",
      "\n",
      "I don't have the capability to remember personal information like names. However, I can help you find information using the tavily_search function if you need assistance with something else!\n"
     ]
    }
   ],
   "source": [
    "# The only difference is we change the `thread_id` here to \"2\" instead of \"1\"\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    {\"configurable\": {\"thread_id\": \"2\"}},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
